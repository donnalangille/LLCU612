{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Collocates with Jane Austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions for my mini-assignment 5 are as follows: In Jupyter, using the NLTK Library, complete the following steps to build a concordance for the word “love” from the Jane Austen corpus.\n",
    "\n",
    "1. Import the Jane Austen files.\n",
    "\n",
    "2. Tokenize all of the words.  \n",
    "\n",
    "3. Create a dictionary for word frequencies.\n",
    "\n",
    "            3a. What are the top 10 frequency words (not including stop words)?   \n",
    "\n",
    "            3b. Are the top frequency words the same as in Voyant?\n",
    "\n",
    "3. Create a concordance (10 lines) for the word “love”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "textFiles = glob.glob(\"austen/*txt\")\n",
    "textFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is copy and pasted from my previous mini-assignment. One of the challenges that I had with this assignment is that I seem to be only able to loop through Persuasion. However, in the following code, I'm able to count all of the characters of each text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCharacters = 0\n",
    "for textFile in textFiles: \n",
    "    f = open(textFile, \"r\",encoding=\"utf-8-sig\") \n",
    "    textString = f.read() \n",
    "    f.close() \n",
    "    chars = len(textString) \n",
    "    print(textFile, \"has\", chars, \"characters\") \n",
    "    totalCharacters += chars \n",
    "print(\"total characters: \", totalCharacters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am able to tokenize all of the words using the NLTK word tokenize function. However, this is where I start having trouble. Persuasion is the first word that appears but I know that the first text it should loop through is actually Northanger Abbey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for textFile in textFiles: #for loop to count words in a text\n",
    "    f = open(textFile, \"r\",encoding=\"utf-8-sig\") #opens the text and stores as f\n",
    "    textString = f.read() #stores text from f in textString\n",
    "    austenTokens = nltk.word_tokenize(textString.lower()) #change all tokens to lowercase \n",
    "    f.close()\n",
    "austenTokens[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this hiccup, I am still able to perform all of the other tasks for mini-assignment 5; however, only on the Persuasion text. I spent a lot of time trying to figure out why I was not looping through all of the Austen texts, including trying other people's (more successful) code but it still didn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenRealWordTokensSample = [word for word in austenTokens[:10] if word[0].isalpha()]\n",
    "austenRealWordFrequenciesSample = nltk.FreqDist(austenRealWordTokensSample)\n",
    "austenRealWordFrequenciesSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a dictionary for word frequencies. I like the tabulate function because it is easier to read. I am only showing a table for the token sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenRealWordFrequenciesSample.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list of stop words is not part of the assignment, but I wanted to try it anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(sorted(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sample of tokens removes words from the stopword list (e.g. \"by\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample words: \", austenRealWordTokensSample)\n",
    "print(\"sample words not in stopwords list: \", [word for word in austenRealWordTokensSample if not word in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed all of the letters to lowercase and created another table based on word frequencies. Now it becomes really apparent that I'm only looping through Persuasion. I thought about adding \"could\" or \"would\" to the stoplist but they might tell us something about the text. I haven't read Persuasion but perhaps building a concordance for these words (and/or looking at them specifically in Voyant) might provide more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenRealContentWordTokensLowercase = [word for word in austenTokens \\\n",
    "        if word[0].isalpha() and word not in stopwords]\n",
    "austenRealContentWordFrequencies = nltk.FreqDist(austenRealContentWordTokensLowercase)\n",
    "austenRealContentWordFrequencies.tabulate(10) # show a sample of the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Voyant to look at Persuasion. The most frequent words in the corpus: anne (447); captain (303); mrs (291); mr (256); elliot (254); wentworth (191); good (187); little (176); said (173). Could and would are included on the stopword list so I removed them. Once I did, they became the first and third most used words. Again, I'm not familiar with Persuasion or 18th century literature so maybe their frequency isn't significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the concordance of \"love\". However, I'm more interested in \"would\" and \"could\" now so I'm going to create a concordance for those words as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenText = nltk.Text(austenTokens)\n",
    "austenText.concordance(\"love\", lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance for \"could\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenText = nltk.Text(austenTokens)\n",
    "austenText.concordance(\"could\", lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance for \"would\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austenText = nltk.Text(austenTokens)\n",
    "austenText.concordance(\"would\", lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initally wanted to build a concordance for \"love\" using the whole Austen corpus, but due to an error in my code I was only able to build a concordance using Persuasion. This exercise demonstrated the importance of stopword lists. The researchers' decision to include, or not include, specific words can potentially have a great effect on the outcomes of performing textual anaylsis on a corpus. My observation of the high frequency words \"would\" and \"could\" might suggest something about the text. To explore further, I could look at the other Austen texts, or other 18th century novels, to see if this word frequency is unique to this novel. If it is unique, I would like to see what words collocate with \"would\" and \"could\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
